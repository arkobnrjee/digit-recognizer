{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae52b28b",
   "metadata": {},
   "source": [
    "# Digit Recognizer\n",
    "\n",
    "My first, highly experimentative attempt at creating a digit recognizer using a neural network with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f817384",
   "metadata": {},
   "source": [
    "## Base Neural Network Class\n",
    "\n",
    "The NeuralNetwork class is very simple. It initializes and holds the neural network, is capable of evaluating the network at a given point, and executing back propagation on a given set of data. This gives the client class quite a bit of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, network_architecture, regularization_constant=0.0, learning_rate=0.1):\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(network_architecture)\n",
    "        self.network_architecture = network_architecture\n",
    "        # Here, weights[layer][i] gives the list of weights fed into the ith node of the given layer.\n",
    "        self.weights = self._create_weight_array()\n",
    "        # Here biases[layer][i] is the bias for the ith node of the given layer.\n",
    "        self.biases = self._create_node_array()\n",
    "\n",
    "    def _create_weight_array(self, randomize=True):\n",
    "        arr = [None]\n",
    "        for layer in range(1, self.num_layers):\n",
    "            # We use normalized Xavier initialization.\n",
    "            if randomize:\n",
    "                randomizer = np.vectorize(lambda x: (np.random.random() - 0.5) * 2 * np.sqrt(6)\n",
    "                    / np.sqrt(self.network_architecture[layer - 1] + self.network_architecture[layer]))\n",
    "                arr.append(randomizer(\n",
    "                    np.empty((self.network_architecture[layer], self.network_architecture[layer - 1]))\n",
    "                ))\n",
    "            else:\n",
    "                arr.append(np.zeros((self.network_architecture[layer], self.network_architecture[layer - 1])))\n",
    "        return arr\n",
    "\n",
    "    def _create_node_array(self, randomize=True):\n",
    "        arr = [None]\n",
    "        for layer in range(1, self.num_layers):\n",
    "            # We use normalized Xavier initialization.\n",
    "            if randomize:\n",
    "                randomizer = np.vectorize(lambda x: ((np.random.random() - 0.5) * 2 * np.sqrt(6)\n",
    "                    / np.sqrt(self.network_architecture[layer - 1] + self.network_architecture[layer])))\n",
    "                arr.append(randomizer(\n",
    "                    np.empty((self.network_architecture[layer]))\n",
    "                ))\n",
    "            else:\n",
    "                arr.append(np.zeros((self.network_architecture[layer])))\n",
    "        return arr\n",
    "\n",
    "    def compute_output_layers(self, input_layers):\n",
    "        curr_layers = np.transpose(input_layers)\n",
    "        for layer in range(1, self.num_layers):\n",
    "            curr_layers = np.tanh(self.weights[layer] @ curr_layers\n",
    "                + self.biases[layer].reshape(self.network_architecture[layer], 1))\n",
    "        return np.transpose(curr_layers)\n",
    "\n",
    "    def sgd(self, input_pts, output_pts, batch_size=1, num_epochs=1, log=True):\n",
    "        num_pts = len(input_pts)\n",
    "        num_batches = int(np.ceil(num_pts / batch_size))\n",
    "        for epoch in range(num_epochs):\n",
    "            if log:\n",
    "                print(\"Epoch {} / {}\".format(epoch + 1, num_epochs))\n",
    "            order = np.arange(num_pts)\n",
    "            np.random.shuffle(order)\n",
    "            for batch in range(num_batches):\n",
    "                self._back_prop(input_pts[batch * batch_size:(batch + 1) * batch_size],\n",
    "                                output_pts[batch * batch_size:(batch + 1) * batch_size])\n",
    "    \n",
    "    def _back_prop(self, input_pts, output_pts):\n",
    "        num_datapts = len(input_pts)\n",
    "        weight_gradient = self._create_weight_array(randomize=False)\n",
    "        bias_gradient = self._create_node_array(randomize=False)\n",
    "        for input_layer, output_layer in zip(input_pts, output_pts):\n",
    "            self._add_gradient(input_layer, output_layer, weight_gradient, bias_gradient)\n",
    "        for curr_weight_vector, delta_weight in zip(self.weights, weight_gradient):\n",
    "            if curr_weight_vector is not None:\n",
    "                curr_weight_vector *= 1 - 2 * self.learning_rate * self.regularization_constant / num_datapts\n",
    "                curr_weight_vector -= self.learning_rate * delta_weight / num_datapts\n",
    "        for curr_bias_vector, delta_bias in zip(self.biases, bias_gradient):\n",
    "            if curr_bias_vector is not None:\n",
    "                curr_bias_vector *= 1 - 2 * self.learning_rate * self.regularization_constant / num_datapts\n",
    "                curr_bias_vector -= self.learning_rate * delta_bias / num_datapts\n",
    "\n",
    "    def _add_gradient(self, input_layer, output_layer, weight_gradient, bias_gradient):\n",
    "        neuron_values = self._get_all_layers(input_layer)\n",
    "        node_value_derivative = self._create_node_array(randomize=False)\n",
    "        node_inner_value_derivative = self._create_node_array(randomize=False)\n",
    "        for layer in range(self.num_layers - 1, 0, -1):\n",
    "            curr_neuron_values = neuron_values[layer]\n",
    "            if layer == self.num_layers - 1:\n",
    "                # This is the output layer.\n",
    "                output_layer_size = self.network_architecture[-1]\n",
    "                node_value_derivative[layer] = 2 / output_layer_size * (curr_neuron_values - output_layer)\n",
    "            else:\n",
    "                node_value_derivative[layer] = (np.transpose(self.weights[layer + 1])\n",
    "                                                @ node_inner_value_derivative[layer + 1])\n",
    "            node_inner_value_derivative[layer] = ((1 - curr_neuron_values * curr_neuron_values)\n",
    "                                                  * node_value_derivative[layer])\n",
    "            curr_layer_size = self.network_architecture[layer]\n",
    "            prev_layer_size = self.network_architecture[layer - 1]\n",
    "            curr_inner_deriv = node_inner_value_derivative[layer].reshape((curr_layer_size, 1))\n",
    "            prev_vals = neuron_values[layer - 1].reshape((1, prev_layer_size))\n",
    "            weight_gradient[layer] += curr_inner_deriv @ prev_vals\n",
    "            bias_gradient[layer] += node_inner_value_derivative[layer]\n",
    "\n",
    "    def _get_all_layers(self, input_layer):\n",
    "        layers = [input_layer]\n",
    "        curr_layer = input_layer\n",
    "        for layer in range(1, self.num_layers):\n",
    "            curr_layer = np.tanh(self.weights[layer] @ curr_layer + self.biases[layer])\n",
    "            layers.append(curr_layer)\n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058359b",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "In order to find the optimal parameters, we implement validation with a fifth of the training data. I was initially going to use 10-fold cross validation, but it proved to be too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a tuple of arrays.\n",
    "# First element is the array of validation errors after each epoch.\n",
    "# Second element is the array of validation accuracies after each epoch.\n",
    "\n",
    "\n",
    "def calc_errors(input_pts, output_pts, network_architecture,\n",
    "                regularization_constant=0.0, learning_rate=0.1,\n",
    "                num_epochs=30, batch_size=1, log=True):\n",
    "    num_pts = len(input_pts)\n",
    "    num_train_pts = num_pts * 4 // 5\n",
    "    num_validation_pts = num_pts - num_train_pts\n",
    "    training_inputs = input_pts[:num_train_pts]\n",
    "    validation_inputs = input_pts[num_train_pts:]\n",
    "    training_outputs = output_pts[:num_train_pts]\n",
    "    validation_outputs = output_pts[num_train_pts:]\n",
    "    network = NeuralNetwork(\n",
    "        network_architecture,\n",
    "        regularization_constant=regularization_constant,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    errors = np.empty(num_epochs)\n",
    "    accuracies = np.zeros(num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        if log:\n",
    "            print(\"Epoch {} / {}:\".format(epoch + 1, num_epochs))\n",
    "        network.sgd(training_inputs, training_outputs,\n",
    "                    batch_size=batch_size, log=False)\n",
    "        validation_results = network.compute_output_layers(validation_inputs)\n",
    "        errors[epoch] = np.average(np.square(validation_results - validation_outputs))\n",
    "        for validation_out, validation_exp_out in zip(validation_results, validation_outputs):\n",
    "            accuracies[epoch] += (np.argmax(validation_out) == np.argmax(validation_exp_out))\n",
    "        accuracies[epoch] /= num_validation_pts\n",
    "        if log:\n",
    "            print(\"Validation Error: \" + str(errors[epoch]))\n",
    "            print(\"Accuracy: \" + str(accuracies[epoch]))\n",
    "    return errors, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d8260",
   "metadata": {},
   "source": [
    "## Testing Parameters\n",
    "\n",
    "At this point, we fetch the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8c6cb-3c95-4075-8cbf-e6e701b67dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = len(data)\n",
    "input_pts = data.iloc[:, 1:].to_numpy() * 2 / 255 - 1\n",
    "labels = data.iloc[:, 0].to_numpy()\n",
    "output_pts = np.full((num_pts, 10), -1)\n",
    "for image_index, label in enumerate(labels):\n",
    "    output_pts[image_index,label] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f3e6d",
   "metadata": {},
   "source": [
    "We now graph out validation error and accuracy for a few sample models with different parameters. First, we vary the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for learning_rate in [0.1, 0.2, 0.3, 0.4, 0.5, 1]:\n",
    "    print(\"Learning rate: {}\".format(learning_rate))\n",
    "    errors, accuracies = calc_errors(input_pts, output_pts, (784, 64, 64, 10), learning_rate=learning_rate, batch_size=128, num_epochs=5)\n",
    "    plt.title(\"Learning rate: {}\".format(learning_rate))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.plot(errors)\n",
    "    plt.show()\n",
    "    plt.title(\"Learning rate: {}\".format(learning_rate))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(accuracies)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3a47b-8d8b-479d-afa5-257bea81ee3c",
   "metadata": {},
   "source": [
    "It seems like a learning rate of 0.2 is good. Now, we vary the regularization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82696d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rc in [0, 1e-4, 1e-3, 1e-2, 1e-1, 2e-1]:\n",
    "    errors, accuracies = calc_errors(input_pts, output_pts, (784, 64, 64, 10),\n",
    "                                     learning_rate=0.2, batch_size=128, num_epochs=30,\n",
    "                                     regularization_constant=rc)\n",
    "    print(\"Regularization: {}\".format(rc))\n",
    "    plt.title(\"Regularization: {}\".format(rc))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.plot(errors)\n",
    "    plt.show()\n",
    "    plt.title(\"Regularization: {}\".format(rc))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(accuracies)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59355874-d91b-4587-bfa8-0940f1503010",
   "metadata": {},
   "source": [
    "Regularization doesn't seem to make a difference. We test the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22121fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors, accuracies = calc_errors(input_pts, output_pts, (784, 64, 64, 10), learning_rate=.2, batch_size=128, num_epochs=150)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f3d9a-3f8d-4637-a225-f30a7d05afb7",
   "metadata": {},
   "source": [
    "Now, we create our final model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = NeuralNetwork((784, 64, 64, 10), learning_rate=0.2)\n",
    "final_model.sgd(input_pts, output_pts, batch_size=128, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a74a7b-536c-4866-aa58-f6a3189156f8",
   "metadata": {},
   "source": [
    "Get the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e85e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "num_test_pts = len(test_data)\n",
    "input_test_pts = test_data.to_numpy() * 2 / 255 - 1\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdeb63b-fbf0-4cc1-8db4-1c802be534fe",
   "metadata": {},
   "source": [
    "Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output_layers = final_model.compute_output_layers(input_test_pts)\n",
    "generated_answers = [np.argmax(layer) for layer in generated_output_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d85219-9288-455b-b5da-2c9a20d55cd4",
   "metadata": {},
   "source": [
    "Write output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88020356",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = pd.DataFrame({\"ImageID\" : np.arange(1, num_test_pts + 1),\n",
    "                                 \"Label\" : generated_answers})\n",
    "output_dataframe.to_csv(\"answers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
